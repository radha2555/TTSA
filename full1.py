# -*- coding: utf-8 -*-
"""full1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMwDHMHnjfp4emahUEMnOOOmsfw0iri3
"""

!pip install git+https://github.com/openai/whisper.git
!sudo apt update && sudo apt install ffmpeg

import os
import shutil
import glob

# Define directories
audio_dir = "/content/data"
output_dir = "/content/Transcription_files"

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Get all audio files in the directory
audio_files = [f for f in os.listdir(audio_dir) if f.endswith((".aac", ".wav", ".mpeg"))]

# Loop through each audio file and transcribe it
for audio_file in audio_files:
    input_path = os.path.join(audio_dir, audio_file)
    base_filename = os.path.splitext(audio_file)[0]  # Filename without extension

    # Run Whisper with output directory specified
    os.system(f'whisper "{input_path}" --model large-v2 --language en --output_dir "{output_dir}"')

    # Get the generated .txt file path
    final_output_txt = os.path.join(output_dir, base_filename + ".txt")

    # Delete all other files except .txt
    for file in glob.glob(os.path.join(output_dir, base_filename + ".*")):
        if not file.endswith(".txt"):
            os.remove(file)

    # Confirm if the transcription was successful
    if os.path.exists(final_output_txt):
        print(f"✅ Transcription for {audio_file} saved to {final_output_txt}")
    else:
        print(f"❌ Transcription for {audio_file} failed or .txt file not found.")


!pip install transformers torch
import os
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import torch
import re

# Load sentiment analysis model
sentiment_model_name = "distilbert-base-uncased-finetuned-sst-2-english"
sentiment_analyzer = pipeline("sentiment-analysis", model=sentiment_model_name)

# Load model for customer interest detection (you can replace this with a fine-tuned model for better results)
intent_model_name = "bert-base-uncased"  # Or another model that is more specific to intent analysis
tokenizer = AutoTokenizer.from_pretrained(intent_model_name)
intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_name)

# Load summarization model (you can use T5, BART, etc.)
summarizer = pipeline("summarization")

def load_transcription(file_path):
    """
    Load transcription text from a file.
    """
    try:
        with open(file_path, 'r') as file:
            transcription_text = file.read()
        return transcription_text
    except FileNotFoundError:
        print(f"File at {file_path} not found.")
        return None

def analyze_sentiment_and_interest(text):
    """
    Analyze sentiment and detect customer interest directly from the transcription text.
    """
    # Sentiment analysis
    # Truncate the input text to the maximum sequence length of the model
    sentiment_result = sentiment_analyzer(text, truncation=True, max_length=512)[0]  # Added truncation
    sentiment = sentiment_result['label']  # Positive or Negative

    # Customer Interest detection (using a simple keyword-based approach, can be improved)
    keywords_interest = ["interested", "buy", "sign up", "call me", "details", "more info", "follow up"]
    keywords_disinterest = ["not interested", "don't want", "no", "leave me alone", "DNB"]

    if any(word in text.lower() for word in keywords_interest):
        customer_interest = "Interested"
    elif any(word in text.lower() for word in keywords_disinterest):
        customer_interest = "Not Interested"
    else:
        customer_interest = "Neutral"

    return sentiment, customer_interest

def summarize_conversation(text):
    """
    Summarize the transcription text.
    """
    # Use the summarizer pipeline to generate a summary
    # Applying truncation and setting max_length for the summarizer
    try:
        summarized_text = summarizer(text, max_length=150, min_length=50, do_sample=False, truncation=True)
        return summarized_text[0]['summary_text']
    except IndexError:
        # If the summarization fails due to text length or unexpected content,
        # return a message indicating the issue
        return "Could not generate a summary for this conversation."

def process_transcription(file_path):
    """
    Main function to read the transcription from file, analyze sentiment and interest, and summarize it.
    """
    # Load the transcription text from file
    transcription_text = load_transcription(file_path)

    if not transcription_text:
        return {"error": "No transcription text found"}

    # Analyze sentiment and customer interest
    sentiment, customer_interest = analyze_sentiment_and_interest(transcription_text)

    # Summarize the conversation
    summary = summarize_conversation(transcription_text)

    # Return the analysis results
    return {
        "sentiment": sentiment,
        "customer_interest": customer_interest,
        "summary": summary
    }

def process_multiple_files(directory_path):
    """
    Process all transcription files in a given directory.
    """
    # Get list of all .txt files in the directory
    transcription_files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]

    # Initialize a results dictionary to store the results for each file
    results = {}

    # Loop through each file in the directory
    for file_name in transcription_files:
        file_path = os.path.join(directory_path, file_name)

        # Process the transcription file
        result = process_transcription(file_path)

        # Store the result in the dictionary with file name as the key
        results[file_name] = result

    # Return all the results
    return results

# Example usage: Path to your transcription files folder
directory_path = '/content/Transcription_files'  # Adjust this path to your folder with .txt files

# Process all transcription files in the directory
all_results = process_multiple_files(directory_path)

# Output the results for each file
for file_name, result in all_results.items():
    print(f"Results for {file_name}:")
    print("Sentiment:", result.get('sentiment'))
    print("Customer Interest:", result.get('customer_interest'))
    print("Summary:", result.get('summary'))
    print("-" * 50)

import os
import shutil
import glob
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from docx import Document

# Define directories
audio_dir = "/content/data"
output_dir = "/content/Transcription_files"
doc_output_path = "/content/sentiment_analysis_results.docx"

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Load sentiment analysis model
sentiment_model_name = "distilbert-base-uncased-finetuned-sst-2-english"
sentiment_analyzer = pipeline("sentiment-analysis", model=sentiment_model_name)

# Load model for customer interest detection (you can replace this with a fine-tuned model for better results)
intent_model_name = "bert-base-uncased"  # Or another model that is more specific to intent analysis
tokenizer = AutoTokenizer.from_pretrained(intent_model_name)
intent_model = AutoModelForSequenceClassification.from_pretrained(intent_model_name)

# Load summarization model (you can use T5, BART, etc.)
summarizer = pipeline("summarization")

# Create or open the .docx file to store sentiment analysis results
doc = Document()
doc.add_heading('Sentiment Analysis Results', 0)

def load_transcription(file_path):
    """
    Load transcription text from a file.
    """
    try:
        with open(file_path, 'r') as file:
            transcription_text = file.read()
        return transcription_text
    except FileNotFoundError:
        print(f"File at {file_path} not found.")
        return None

def analyze_sentiment_and_interest(text):
    """
    Analyze sentiment and detect customer interest directly from the transcription text.
    """
    # Sentiment analysis
    sentiment_result = sentiment_analyzer(text, truncation=True, max_length=512)[0]  # Added truncation
    sentiment = sentiment_result['label']  # Positive or Negative

    # Customer Interest detection (using a simple keyword-based approach, can be improved)
    keywords_interest = ["interested", "buy", "sign up", "call me", "details", "more info", "follow up"]
    keywords_disinterest = ["not interested", "don't want", "no", "leave me alone", "DNB"]

    if any(word in text.lower() for word in keywords_interest):
        customer_interest = "Interested"
    elif any(word in text.lower() for word in keywords_disinterest):
        customer_interest = "Not Interested"
    else:
        customer_interest = "Neutral"

    return sentiment, customer_interest

def summarize_conversation(text):
    """
    Summarize the transcription text.
    """
    try:
        summarized_text = summarizer(text, max_length=150, min_length=50, do_sample=False, truncation=True)
        return summarized_text[0]['summary_text']
    except IndexError:
        return "Could not generate a summary for this conversation."

def process_transcription(file_path):
    """
    Main function to read the transcription from file, analyze sentiment and interest, and summarize it.
    """
    transcription_text = load_transcription(file_path)

    if not transcription_text:
        return {"error": "No transcription text found"}

    sentiment, customer_interest = analyze_sentiment_and_interest(transcription_text)

    summary = summarize_conversation(transcription_text)

    # Store results in the doc file
    doc.add_heading(f"Results for {os.path.basename(file_path)}", level=1)
    doc.add_paragraph(f"Sentiment: {sentiment}")
    doc.add_paragraph(f"Customer Interest: {customer_interest}")
    doc.add_paragraph(f"Summary: {summary}")
    doc.add_paragraph("-" * 50)

    return {
        "sentiment": sentiment,
        "customer_interest": customer_interest,
        "summary": summary
    }

def process_multiple_files(directory_path):
    """
    Process all transcription files in a given directory.
    """
    transcription_files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]

    results = {}

    for file_name in transcription_files:
        file_path = os.path.join(directory_path, file_name)

        # Skip files that have already been processed
        if os.path.exists(file_path):
            results[file_name] = process_transcription(file_path)

    # Save the document with sentiment analysis results
    doc.save(doc_output_path)

    return results

def transcribe_files(directory_path):
    """
    Transcribe audio files if they haven't been transcribed yet.
    """
    audio_files = [f for f in os.listdir(audio_dir) if f.endswith((".aac", ".wav", ".mp3"))]

    for audio_file in audio_files:
        input_path = os.path.join(audio_dir, audio_file)
        base_filename = os.path.splitext(audio_file)[0]  # Filename without extension
        final_output_txt = os.path.join(output_dir, base_filename + ".txt")

        # Skip transcription if the file is already transcribed
        if os.path.exists(final_output_txt):
            print(f"Skipping transcription for {audio_file} as it has already been processed.")
            continue

        # Run Whisper for transcription
        os.system(f'whisper "{input_path}" --model base --language en --output_dir "{output_dir}"')

        # Get the generated .txt file path
        if os.path.exists(final_output_txt):
            print(f"✅ Transcription for {audio_file} saved to {final_output_txt}")
        else:
            print(f"❌ Transcription for {audio_file} failed or .txt file not found.")

# Example usage: Path to your transcription files folder
directory_path = '/content/Transcription_files'

# Transcribe files first if they haven't been transcribed
transcribe_files(directory_path)

# Process all transcription files in the directory
all_results = process_multiple_files(directory_path)

# Output the results for each file
for file_name, result in all_results.items():
    print(f"Results for {file_name}:")
    print("Sentiment:", result.get('sentiment'))
    print("Customer Interest:", result.get('customer_interest'))
    print("Summary:", result.get('summary'))
    print("-" * 50)

