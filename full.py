# -*- coding: utf-8 -*-
"""FULL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBPDQTPYrVZ_g0oyLbX4wR_7W0_werlg
"""

!pip install git+https://github.com/openai/whisper.git
!sudo apt update && sudo apt install ffmpeg

import os
import glob

# Define directories
audio_dir = "/content/data"
output_dir = "/content/Transcription_files"

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Get all audio files in the directory
audio_files = [f for f in os.listdir(audio_dir) if f.endswith((".aac", ".wav", ".mp4", ".mp3"))]

# Loop through each audio file and transcribe it if not already done
for audio_file in audio_files:
    input_path = os.path.join(audio_dir, audio_file)
    base_filename = os.path.splitext(audio_file)[0]  # Filename without extension
    final_output_txt = os.path.join(output_dir, base_filename + ".txt")

    # Check if transcription already exists
    if os.path.exists(final_output_txt):
        print(f"✅ Skipping {audio_file}, transcription already exists at {final_output_txt}")
        continue  # Skip to the next file

    # Run Whisper with output directory specified
    os.system(f'whisper "{input_path}" --model large-v2 --language en --output_dir "{output_dir}"')

    # Delete all other files except .txt
    for file in glob.glob(os.path.join(output_dir, base_filename + ".*")):
        if not file.endswith(".txt"):
            os.remove(file)

    # Confirm if the transcription was successful
    if os.path.exists(final_output_txt):
        print(f"✅ Transcription for {audio_file} saved to {final_output_txt}")
    else:
        print(f"❌ Transcription for {audio_file} failed or .txt file not found.")

!pip install langchain_groq
!pip install langchain-community

import os
import warnings
import logging
import tempfile  # Ensure tempfile is imported
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader, DirectoryLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain.chains import RetrievalQA

# Disable warnings and info logs
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)

# Function to extract text content from all files in a folder
def extract_text_from_folder(folder_path):
    texts = []
    files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.txt')]

    for file_path in files:
        with open(file_path, 'r', encoding='utf-8') as file:
            texts.append(file.read())  # Add file content to the list
    return "\n".join(texts), files  # Combine all texts and return file list

# Function to perform summarization and sentiment analysis
def analyze_text(text):
    # Placeholder for sentiment analysis (replace with actual model)
    sentiment = "Positive" if "good" in text.lower() else "Negative"

    # Improved Customer Interest Detection
    keywords_interest = ["interested", "buy", "sign up", "call me", "details", "more info", "follow up"]
    keywords_disinterest = ["not interested", "don't want", "no", "leave me alone", "DNB"]

    if any(word in text.lower() for word in keywords_interest):
        customer_interest = "Interested"
    elif any(word in text.lower() for word in keywords_disinterest):
        customer_interest = "Not Interested"
    else:
        customer_interest = "Neutral"

    # Use LLM for summarization
    summarization_prompt = f"Summarize the following content:\n\n{text}"
    summary = get_answer_from_model(summarization_prompt)

    return sentiment, customer_interest, summary

# Function to initialize the vectorstore with multiple files
def get_vectorstore(folder_path):
    document_text, files = extract_text_from_folder(folder_path)  # Extract text from all files

    # Create a temporary file to store the combined content
    with tempfile.NamedTemporaryFile(delete=False, mode='w', encoding='utf-8') as temp_file:
        temp_file.write(document_text)
        temp_file_path = temp_file.name

    # Use TextLoader to load all text files from the folder
    loader = DirectoryLoader(folder_path, glob="*.txt", loader_cls=TextLoader)
    index = VectorstoreIndexCreator(
        embedding=HuggingFaceEmbeddings(model_name='all-MiniLM-L12-v2'),
        text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    ).from_loaders([loader])

    # Clean up the temporary file
    os.remove(temp_file_path)

    return index.vectorstore

# Function to process the user query and generate a response using RAG
def get_answer_from_model(prompt):
    try:
        # Initialize the Groq model
        model = "llama3-8b-8192"  # Adjust model as per your requirement

        # Set the API key as an environment variable
        os.environ["GROQ_API_KEY"] = "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"

        groq_chat = ChatGroq(
            groq_api_key=os.environ.get("GROQ_API_KEY"),
            model_name=model
        )

        # Initialize RetrievalQA chain
        folder_path = "/content/Transcription_files"  # Set the folder containing the text files
        vectorstore = get_vectorstore(folder_path)
        chain = RetrievalQA.from_chain_type(
            llm=groq_chat,
            chain_type='stuff',
            retriever=vectorstore.as_retriever(search_kwargs={'k': 3}),
            return_source_documents=True
        )

        # Run the RAG-based model
        result = chain({"query": prompt})
        response = result["result"]

        return response

    except Exception as e:
        return f"Error: {str(e)}"

# Main function to process all text files in a folder
def process_folder(folder_path):
    all_results = {}

    document_text, files = extract_text_from_folder(folder_path)

    for file in files:
        text = extract_text_from_files([file])

        # Analyze the text for sentiment, interest, and summary
        sentiment, customer_interest, summary = analyze_text(text)

        # Store results
        all_results[file] = {
            'sentiment': sentiment,
            'customer_interest': customer_interest,
            'summary': summary
        }

    return all_results

# Example usage: Replace with actual folder path
folder_path = "/content/Transcription_files"
results = process_folder(folder_path)

# Output the results
for file_name, result in results.items():
    print(f"Results for {file_name}:")
    print("Sentiment:", result.get('sentiment'))
    print("Customer Interest:", result.get('customer_interest'))
    print("Summary:", result.get('summary'))
    print("-" * 50)